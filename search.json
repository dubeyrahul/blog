[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A place to put down my thoughts on Python, Machine Learning, AI, and Distributed Systems."
  },
  {
    "objectID": "posts/effective-python-1/2019-08-03-writing-pythonic-code.html",
    "href": "posts/effective-python-1/2019-08-03-writing-pythonic-code.html",
    "title": "Writing Pythonic Code: Part 1",
    "section": "",
    "text": "This is the first of many Python related posts that I plan to write while studying how to write good Python code from the book Effective Python. In this post, we will go over some techniques of making your Python code more readable by following some simple Pythonic principles laid out in PEP-8."
  },
  {
    "objectID": "posts/effective-python-1/2019-08-03-writing-pythonic-code.html#conclusion",
    "href": "posts/effective-python-1/2019-08-03-writing-pythonic-code.html#conclusion",
    "title": "Writing Pythonic Code: Part 1",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we saw several techniques to make your code more Pythonic, more readable, clear, and concise. We learned that we can apply most of these techniques by using automated tools and make our code more Pythonic. In the next post we’ll go over writing effective Python code."
  },
  {
    "objectID": "posts/llm-from-scratch/attention.html",
    "href": "posts/llm-from-scratch/attention.html",
    "title": "LLMs Part 2: Attention",
    "section": "",
    "text": "In this post, I’ll dive into the attention mechanism that is one of the key feature of modern LLMs. We’ll go over some of the shortcomings of pre-LLM Neural language models such as RNNs and its variants, how attention solves these shortcomings, and how it is implemented in practice. Lastly, we’ll discuss what are some computational infrastructure implication of attention mechanism that allows large scale training.\n\nNeural language models\nLanguage models are nothing but an ML model tasked with modeling the language, simply put we want to learn a probability distribution over the language. We want to be able to predict P(word | context-words). The initial approaches were doing so were very specific to textual data that leveraged the grammar and structure of the language. Next, came statistical approaches such as Naive Bayes / Bag-of-words model. Then we got one of the most impressive and cited paper: A Neural Probabilistic Language Model where the authors demonstrated how to leverage MLPs to model the language. Next, we got word embedding based model but after MLP the next big advancement came in Sequence to Sequence Learning with Neural Networks which proposed LSTMs with encoder-decoder setup to encode and decode sequences and were particularly impressive in tasks such as language translation. For quite a long period, these RNN-variants were the state of the art approach but they suffered from the following issues: - unable to model long range dependency - slow training due to serial processing of text - vanishing and exploding gradients\nTo solve some of these problems, the next paper Attention is all you need created a new architecture called Transformer which leveraged Attention. The concept of attention was not introduced in this paper but in another paper: Neural Machine Translation by Jointly Learning to Align and Translate.\n\n\nProblems solved by Attention mechanism\nThe way RNN based models work is they process one token at a time and as they move forward in time, they create a hidden representation of context seen so far. It does not refer to older tokens directly but only relies on this learned hidden representation. Since, they are using a single vector to encode the entire historical text, it is difficult for them to encode long range dependency.\nAttention mechanism gets around this problem by basically doing a “smart” brute-force approach. Rather than summarizing all the historical context into a single vector, we let each to-be-generated token to go back and look at all the historical tokens directly! Why restrict it to a compressed representation when I can look at all the previous tokens and their representation? Moreover, during training it learns how much “attention” to pay each historical token to generate the current token: hence the “smart” brute-force.\nBy thinking this way, we can see how incredibly more powerful attention mechanism can be: it has access to more information directly compared to traditional RNN based approaches. Now, one may think this brute force approach is great but must be super slow compared to RNN right? RNN is only using one historical vector whereas attention is using so many! To get around this problem, we use parallelization. Since attention works by doing a bunch of vector multiplication between current token and historical token, you can set it up as a matrix multiplication and leverage GPUs specialized for parallelizing matrix multiplication operations! So, in theory, yes we are doing a lot more computation than RNNs but we are able to do it really fast because the operations can be parallelized, whereas in RNN, where each token is processed sequentially, we cannot parallelize it.\nNow, the gradient problem with RNN was due to gradient being propagated from the last token to the first token via a bunch of multiplication steps. When a bunch of small numbers or large numbers are multiplied, they either reach 0 or a really large number, both of which are terrible for back propagation. In attention, we have direct connection between tokens so the information (gradient) can directly propagate from the last token to the first token: no multiplication-hops required! To be fair, in addition to this attention mechanism, some nice techniques such as Layer Normalization and Residual connections were added to the Transformer architecture to handle the vanishing/exploding gradient problem.\nNow that we know that attention mechanism is awesome and helps us build Transformers, which are backbone of all modern LLMs (at least in 2024), let’s dive into the details of attention mechanism and its variants.\n\n\nAttention mechanism\nAt its core, attention mechanism allows a model to build a contextual representation of each token by incorporating information from surrounding tokens. Attention really means which surrounding tokens the model should pay more attention to. It can be described as analogous to how we read a text: as we read English text from left-to-right, we remember certain words more than the other to understand the meaning of the sentence.\nAttention mechanism requires us to understand 3 concepts: Queries, Keys, and Values 1. Query: Query is like a search term and represents the current token that the model is trying to understand 2. Key: Key is like a database index used to index and search a database. Query is compared to Key to find which tokens to pay more attention to 3. Values: Value represents the actual input. After determening which keys to pay attention to, we retrieve the values of those tokens in proportion to how relevant their keys were to the query.\nIn essence, attention mechanism figures out how relevant each key is to the query, and then uses this weight to values to compute contextual representation for the query token.\n\nSimple Self-attention\nSelf-attention simply means that the token in the sequence attends to the tokens of the same sequence. It is called self-attention because the first definition of attention in the literature was in the context of encoder-decoder architecture of sequence translation where the decoder was allowed to “attend” to the state of encoder at each token step, therefore the decoder was not attending on itself but on the encoder. In self-attention, the decoder (or encoder) attends to its own state and the sequence input to itself.\nTo understand self-attention, let’s start with a simplified self-attention. Say we have a sequence of token input embeddings [X1, X2, X3,..XT]. Recall from part-1, that these embeddings are based on vocabulary and position, i.e. they are not contextual. Both the sequences: “An apple today”, and “An apple iphone” will have same X for apple. To create a contextual embedding, we want to compare apple to its surrounding words and compute an attention score w.r.t each surrounding token.\nLet’s call these Attention-score(X1) = [W1, W2, W3,...WT]. For now, let’s assume that we can just take dot-product between X1 and all other X_i to get W_i. Next, we normalize this W vector with the softmax function to get W_norm. Now, we can compute contextual vector XC_1 for X1 by multiplying it with normalized attention scores in W_norm and adding up the vectors.\nTherefore, XC_1 = W_norm * X. We just take a weighted-sum of X to get XC.\nIn this naive attention scoring, we have simply taken a dot product of input X with itself (X @ X.T) to get attention scores W, then we softmax normalize it for each token, and lastly multiply this back with X.\n\n\nSelf-attention\nNow, let’s understand the actual self-attention mechanism with weights that LLMs learn. It is quite similar to the above simplified approach, the only difference being (1) how W is calculated and (2) how W is utilized to get the contextual vector. The rest of the operations remain the same: (1) compute attention scores (2) normalize attention scores (3) weight the input per normalized attention scores to get contextual representation.\nComputing Attention weight\nWe ask the LLM to learn 3 matrices: W_Q: Query, W_K: Keys, W_V: Values and these matrices drive the computation of attention scores. Say, we have a token embedding X1. First thing we do is, we bring it into Q's space by multiplying it with W_Q to get X1_Q. Note that to carry out this multiplication, the shape of W_Q should align with X1. So if X1 is of dimension d, then W_Q’s first (or both) dimension has to be d. We will use this X1_Q: query vector to search among the Keys (similar to a database where a query is evaluated against index keys).\nTo do this, we compute keys = X @ W_K where X is input matrix of shape (n_tokens, d) and W_K is our Keys matrix of shape (d, d). So we get keys matrix of shape (n_tokens, d). Now, we will compare X1_Q to each of the rows in keys matrix to figure out which other tokens should we pay attention to, which gives us Attention-scores(X1) of shape (1, n_tokens), so a score for each token.\nAttention-scores(X1) = X1 dot keys\nWe essentially did a weighted brute-force search across all tokens in the sequence to find which other tokens should our token-1 attend to.\nNow, we softmax normalize this. However, the trick is to take Softmax(Attention-scores(X1)/ sqrt(d)). This is called scaled-dot-product attention due to the scaling by sqrt(d). Let’s call this Attention-scores-norm(X1).\nReason behind scaling: Say we are taking softmax over 2 elements z1 and z2 and z1 &gt;&gt;&gt; z2. Now, when we calculate a softmax e^z1 &gt;&gt;&gt;&gt;&gt;&gt; e^z2. Therefore, softmax for z1 will tend to become 1 and softmax for z2 will tend to become 0. This leads to softmax function becoming a step function whose gradients are not well-defined and are nearly close to 0.\n\nNow imagine our context-vectors of 1000s of dimensions whose dot product can grow very large. These large dot products run into the same issue as mentioned above. So, to avoid this learning problem during training, we divide the attention scores (dot products) by sqrt(d) (I am guessing a heuristic) and then take a softmax.\nOk, let’s get back! We have the attention-scores normalized. Now, all we have to do is multiply this with “something” to get contextual representation of X1.\nTo compute this “something”, we use the W_V matrix. We compute values = X @ W_V to get a matrix of shape (n_tokens, d). These represent the value of each token in d-dimensional space. Now, we can multiply the attention scores with values:\nX1_C = Attention-scores-norm(X1) @ values (matrix shape math: (1, n_tokens) * (n_tokens, d) =&gt; (1,d))\nWe computed the context vector of just X1 but we can compute this for all tokens at once using matrix multiplication. These computations can also be parallelized and sped up on GPUs, making attention/transformers such an attractive architecture in modern DL systems.\nLet X be our input token sequence of shape (N, d1). We can initialize W_K, W_Q, W_V of shape (d1, d2). Our context vector will be of dimension d2.\nStep 1: Project into K,V,Q:\n\nkeys = X @ W_K                                                  # (N, d2)\nqueries = X @ W_Q                                               # (N, d2)\nvalues = X @ W_V                                                # (N, d2)\n\nStep 2: Compute scaled dot-product attention scores\nattention_scores = queries @ keys.T                             # (N, N)\nattention_scores_norm = softmax(attention_scores / sqrt(d1))    # (N, N)\n\nStep 3: Compute contextual embedding using values\ncontext_x = attention_scores_norm @ values                      # (N, d2)\n\n\nCausal attention\nNow, let’s create a type of attention that is used in decoder-only architecture: these are models used in text-generation where at a certain point in time, the model can only look back at past-tokens rather than all tokens in a sequence. We can continue using the same scaled dot product attention mechanism with a small twist: Just zero-out the tokens after the current token!\nThis is called Causal attention or Masked attention. Causal because we are only relying on previous tokens to predict next tokens so we are saying that the previous tokens causes the next token. I am not sure whether this can be really be called causal from a causal inference standpoint. It is also called Masked because we are masking tokens appearing after the current token so that we only attend to tokens occurring before the current token.\nWe can create a 2D matrix which looks like this:\n[1,0,0,0]\n[1,1,0,0]\n[1,1,1,0]\n[1,1,1,1]\nIt’s a lower-triangular matrix (diagonal and below are 1, rest are 0). It is obvious how this works as a mask. If we multiply this with a matrix, the 0s will 0 out those elements.\nSo, we carry out our attention computation like before, only before we multiply scores with values, we apply this mask so that all the attention-scores after our current token are zeroed-out. We then normalize this masked-attention-score and multiply it with values, and voila! We have causal attention scores for each token!\nLet M be the mask matrix of shape (n, n) where all elements on and below the diagonal are 1, rest are 0.\n\nStep 2: Compute scaled dot-product attention scores and mask it\nattention_scores = queries @ keys.T                                         # (N, N)\nattenion_scores_masked = M @ attention_scores                               # (N, N)\n\n# Normalize the attenion_scores_masked so that rows sum to 1\nattention_scores_norm = attenion_scores_masked / sum_of_rows                # (N, N)\nattention_scores_norm_scaled = softmax(attention_scores_norm / sqrt(d1))    # (N, N)\n\nStep 3: same as before\ncontext_x = attention_scores_norm_scaled @ values                           # (N, d2)\n...\nA slightly better approach would be to think about what softmax does. It performs e^x for each x and divides by sum of each e^x. So, if we set x=-inf, then e^x will automatically be 0. Hence, instead of creating a maxk of 1s and 0s, we can creating a mask of 1s and -inf\nStep 2: Compute scaled dot-product attention scores and mask it\nM = torch.tril(torch.ones(n, n))\nattenion_scores_masked = attention_scores.masked_fill(~mask.bool(), -torch.inf) # replace the 0s (with ~mask.bool) with -inf\nattention_scores_norm_scaled = softmax(attention_scores_norm / sqrt(d1))    # (N, N)\n\nStep 3: same as before\ncontext_x = attention_scores_norm_scaled @ values                           # (N, d2)\nNote: 1. We mask before we normalize, because we want to ensure that attention-scores that are multiplied with values sum to 1. 2. We are still doing things for the entire sequence in parallel by leveraging matrix-multiplication: we still have the same advantages of parallelization that we had in self-attention.\nAnother common operation that is done at this point is applying Dropout to introduce regularization. So the process becomes: 1. Compute attention scores 2. Apply causal mask 4. Softmax with scaling based on d_out 5. Apply dropout (attention weights get scaled by 1 / (1 - dropout_rate) to ensures that rows sum to ~1. For instance, if a particular attention weight is 0.3 before dropout, and dropout is 0.2, the new value after applying dropout will be 0 if that attention weight is dropped, or 0.3 * 1.25 = 0.375 if it is not dropped. The sum of the weights over all inputs may not be exactly 1. This scaling is implemented in Dropout layer and is not specific to Attention, that’s just how Dropout works during training, so that during inference we don’t have to do any scaling.) 6. Compute attention-weighted values\n\n\nMulti-headed attention\nConceptually, multi-headed attention is just the above attention mechanism split into “multiple heads”. Imagine that we want to create a d_out dimensional context vector. We can split this d_out into n_heads where each head is an attention block of d_head = d_out // n_heads. The intuition is that we will train each head independently and the model will learn specific and unique features in each head. It is kind of similar to CNN filters where the idea is that each filter is trained independently and each filter learns something specific and unique about the images. In CNNs, we get one filter that learns edges, another may learn gradients, and so on. In LLMs with multiple-attention heads, we may get one head focusing on syntactic structure, another focus on semantic relationship, and so on. Check out the BertViz tool for a visualization of attention heads.\nNote that, multiple heads is not merely a single massive attention split into multiple heads, i.e. it is really important that each head is trained independently, otherwise we are not learning independent features in each head but just training a single attention head in parallel. This becomes crucial in implementation because we have to lay out and reshape the matrices in such a way that allows for such independent training.\nOne may ask, why not stack attention heads vertically on top of each other rather than next to each other? One neat advantage is that we can achieve similar learning capacity with less cost by laying them out horizontally since it is a single W matrix to learn instead of sequentially learning separate W matrices for each layer.\nImplementation wise, one can essentially create multiple Causal Attention modules and put them in a list such as and compute each head sequentially:\nMultiHeadedAttentionList = concatenate([CA1, CA2, CA3]) # 3 headed causal-attention (CA=causal attention)\nHowever, we can again parallelize this and leverage GPUs to speed things up. Trick to parallelization: stuff it in a matrix such that each head still operates independently but gets computed in parallel.\nMultiHeadedAttention = [CA1_CA2_CA3] # where each CA is laid out next to each other\nThe implementation below is taken from LLM from Scratch In addition to having multiple-heads, we will also introduce a batch dimension so that we are not passing 1 sequence at a time but a batch of sequences. This way we can fully utilize the GPUs!\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.num_heads = num_heads\n        if self.d_out % self.num_heads != 0:\n            raise(\"Num heads should be perfectly divisible by output dimension\")\n        self.head_dim = self.d_out // self.num_heads\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)\n        self.dropout = nn.Dropout(dropout)\n\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        # x @ W.T -&gt; (b, num_tokens, d_in) @ (d_in, d_out) -&gt; (b, num_tokens, d_out)\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # Now, we will split the matrix implicitly by number of heads. We will use view() to do this\n        # now each of keys, queries, values are of shape (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # As discussed above, we want to compute each attention head in parallel, so we will move the dimension around\n        # to make it so -&gt; (b, num_heads, num_tokens, head_dim). We are swapping dimension 1 and 2.\n        # Note that if we ignore the first 2 dimension, we get our simple single headed single sequence attention matrices of size(num-tokens, d_out)\n\n        # now each of keys, queries, values are of shape (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1,2)\n        queries = queries.transpose(1,2)\n        values = values.transpose(1,2)\n\n        # now we compute attention scores for each head and sequence in parallel\n        # we want to multiple queries and keys with matrix multiplication and we want attention scores for each batch, head, and token\n        # desired output dimension is (b, num_heads, num_tokens, num_tokens). So, let's prepare our data to get this.\n        # we will adjust keys so that we get keys shape as (b, num_heads, head_dim, num_tokens) and queries shape is (b, num_heads, num_tokens, head_dim), so the product will have (b, num_heads, num_tokens, num_tokens), i.e. attention for each token with another for each batch, for each head.\n\n        attn_scores = queries @ keys.transpose(2, 3) # we get (b, num_heads, num_tokens, num_tokens)\n\n        # masking\n        mask_bool = self.mask.bool()[:num_tokens,:num_tokens] # because num_tokens can be &lt;  context_length\n        # fill the masked area with -inf\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        # now we can take softmax and see we divide it by sqrt(head_dim)\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # now we compute context-vector which has to be of shape: (b, num_tokens, num_heads, head_dim)\n        # attn_weights shape is (b, num_heads, num_tokens, num_tokens)\n        # values shape is (b, num_heads, num_tokens, head_dim)\n        # We still want to keep computation of context-vector for each head independent so we will do the multiplication as it is\n        # attn_weights @ values will be of shape (b, num_heads, num_tokens, head_dim)\n        # now we can reshape this: (b, num_tokens, num_heads, head_dim)\n\n        context_vec = (attn_weights @ values).transpose(1,2)\n\n        # now that each head has produced context vector independently, we can combine them by stacking horizontally\n        # remember that self.d_out = num_heads * head_dim\n        # we need to do a contiguous operation to lay this out in memory in a contiguous manner so that the view works\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n\n        # the projection multplication is (b, num_tokens, d_out) @ (d_out, d_out) -&gt; (b, num_tokens, d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n        \n        # hence, we get context_vec of shape (b, num_tokens, d_out)\n        return context_vec\n\n\n\nSummary\n\nAttention mechanism allows us to focus on each and every token independently and directly\nAttention scores quantify how much each token should be paid attention to and allows us to “weigh” the token value accordingly\nCausal attention is simply a way to prevent LLM from cheating and looking into the future tokens and we implement it via masking\nMulti-headed attention is several attention heads that are trained independently\nBy leveraging smart layout of the attention heads, we can train each head independently and in parallel at the same time, thereby leveraging GPUs to speed up the code with matrix multiplication instead of sequentially training each head via for loops\nWe can also add Dropout to drop certain attention scores before computing values as a means of regularization.\nBatched Matrix Multiplication can get complicated to get the head around but we can use the following tricks:\n\nWrite down the dimensions of your inputs and desired outputs. Print them if needed while developing modules.\nRemember the following rules of matmul so that we are computing the right output while leveraging parallelism: PyTorch’s torch.matmul (and the @ operator) performs batched matrix multiplication when the tensors have more than two dimensions. The rules are as follows:\n\nIf both tensors are 2D: This is standard matrix multiplication.\nIf one tensor is ND and the other is 2D: The 2D tensor is treated as a batch of matrices, and the multiplication is performed for each batch element of the ND tensor.\nIf both tensors are ND: The multiplication is performed over the last two dimensions of each tensor, with the leading dimensions being treated as batch dimensions."
  },
  {
    "objectID": "posts/effective-python-2/2019-08-04-writing-effective-python-code.html",
    "href": "posts/effective-python-2/2019-08-04-writing-effective-python-code.html",
    "title": "Writing Pythonic code: Part 2",
    "section": "",
    "text": "This is the second post (here’s the first) on writing Pythonic code. In this post, we will first go over some aspects of writing effective Python code by brushing over topics such as: slicing, list comprehension, generator expressions."
  },
  {
    "objectID": "posts/effective-python-2/2019-08-04-writing-effective-python-code.html#conclusion",
    "href": "posts/effective-python-2/2019-08-04-writing-effective-python-code.html#conclusion",
    "title": "Writing Pythonic code: Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we saw several techniques to make our code more efficient and effective. Note that most of these skills build up over time. But it’s helpful to know about them so that we you see them in other people’s code, you can appreciate and understand its intent better."
  },
  {
    "objectID": "posts/feature-store/2020-07-25-feature-stores.html",
    "href": "posts/feature-store/2020-07-25-feature-stores.html",
    "title": "Feature Store: an essential part of ML infrastructure",
    "section": "",
    "text": "In this post, I’ll talk about Feature Store: an essential component of a modern Machine Learning infrastructure. We’ll first briefly look at what a production ML system consists of and where does a Feature Store fit into it. I’ll then dive into some core problems that a Feature Store is supposed to solve and go over some possible solutions to these problems. A combination of these solutions essentially builds up a Feature Store infrastructure.\n\nProduction ML systems:\nFirst, let’s see what differentiates a real-world production ML systems with ML research prototypes, Kaggle based machine learning competitions, and the projects in ML courses. The distinction is best summarized in the diagram below:\n\n\n\nML systems\n\n\nAs we can see, the part where actual ML modeling happens (indicated with ML Code) is a tiny portion of a much larger system. Traditionally, in college courses and online MOOCs a large emphasis is placed on ML Code. I think the primary reason is that it is difficult to teach and assess some of the other skills shown in the diagram. However, the other boxes make up most of what a production-grade ML system looks like.\nIf Machine Learning is a major aspect of your business, then having a strong infrastructure around your ML Code is must. I like to compare it to traditional large scale software systems where it is a must to have a version-controlled code base, CI/CD pipelines, compute and data infrastructure support, etc. for engineers to be effective and for your technical pillar to be strong.\nSince ML is a relatively new software paradigm, a lot of the support around ML systems is either missing or is still developing. As mentioned by Andrej Karpathy in his talk, we need to build a new software stack for AI and ML, and managing data/features (in addition to code) is a big aspect of this new stack.\nIn this post, I want to dive into a specific piece of infrastructure that deals with features: the data that helps you build your ML models and make your ML products successful. At least when it comes to structured data, a large part of ML use-cases is designing good features for your models. However, it doesn’t stop at that. If you are running a business on machine learning, then you want to know the following about your features: - is it available offline for Training and online for Serving - how does one read/load features offline and online - is there consistency between offline and online computations - how good is the quality of features - who is producing these features - who is consuming these features - are there SLAs around some/all of these features - how does one team discover features created by other\nThere are many more such questions that need to be answered about the data goes into your ML models, and the models that power your business. Hence, there is a need for good tooling and infrastructure around features. Feature Store is a relatively new infrastructure solution to answer these questions. Though there are not a lot of mainstream solutions out there, big tech companies in ML space have built some custom infrastructure for this.\nNote: There are now companies like Tecton and Hopsworks who offer end-to-end enterprise grade solutions.\n\n\nFeature Store\nFrom the name it may seem like, we are building a data-store that stores our features, but it is much more than that. Below is a slide taken from Tecton’s talk to highlight (red boxes) the components that a good feature store will manage. \nFor e.g. Serving Infrastructure consists of not just the model that is being served but also the features or feature-transformation logic.\nFeature Store is thus not just a database for you to store and retrieve your features from. It is also the infrastructure to: create those features, manage metadata around the features, make features discoverable, make it shareable, make it consistently available offline and online, monitor the quality of the features, and in some cases even provide consistent API for performing feature engineering.\nNow that we know what a feature store is roughly supposed to do, let’s dive into some core problems that Data scientists (DS) and Machine Learning Engineers (MLE) face and how a good Feature Store can solve those problems.\n\nFeature Consistency\nFeature consistency affects the ML use-cases where predictions are made in real-time, i.e. the predictions are not pre-computed, cached, and served as is without any computation at request time. In real-time serving, we get some request, extract features based on the request, feed it into the model and return the prediction. A fair number of business-driving production use-cases such as: CTR prediction, Search Ranking, Home Feed Ranking, Transaction Fraud Prediction, etc. fall under real-time serving use-case, so this is definitely a problem worth solving. Wait, what exactly is the problem here?\nWhen a DS/MLE builds a model to solve a problem (e.g. CTR prediction), the process usually looks like this: 1. Get data from Data-warehouse or Datalake 2. Do feature engineering: transform your strings to one-hot, compute historical aggregates and so on (usually done in Python-based-stack: Pandas, NumPy, PySpark) 3. Train your model on these features, evaluate for quality 4. Make it available for serving\nAt serving time, you don’t have this full-fledged feature-vector ready, so how does one do feature-engineering/transformation online right before you feed your feature-vector to your model to make prediction. Where does that feature-engineering code live? DS/MLE wrote their feature engineering logic in Python in their notebook but how does it translate to your low-latency Java service? Let’s see some options.\nOption 1: Duplicate the code\nWe can duplicate the code between offline training and online serving systems. Some companies do this by structuring ML teams such that DS write feature engineering code in Python in notebooks, MLE write the same code in Java for serving. In my opinion, this is quite risky especially if your training and serving environments are different (Python v/s Java). Moreover, it’s just bad practice to have duplicate code in different systems having to do the same thing. In addition to that, it’s really difficult to make sure that the computation between offline and online systems are exactly the same. A common way in which this problem surfaces is when your model performs really well offline but does poorly online. A good place to investigate this is to look at the features computed online. Having a good monitoring system to measure offline/online feature-drift also helps, though it doesn’t solve the core problem of having duplicated logic in 2 places in your ML system.\nOption 2: Shared library for computation\nHere, we do not have duplicate code but have a shared library that performs feature engineering. This is doable if your training and serving systems can use the same runtime environment. This becomes tricky with Pandas/NumPy based feature engineering (unless your serving system is in Python). However, this can be done if you use PySpark for feature engineering. PySpark being just a wrapper on top of Scala-Spark essentially performs computations on the JVM. So if your online low-latency serving system is in Java or other JVM-based system, this is a reasonable solution to feature consistency problem. This is what we do at Yelp for our Ads-targeting ML system. For more details, you can check out my blog on Yelp engineering\nOption 3: One compute, Two DB\nHere, we have features computed and loaded into an offline DW/Datalake as well as online database (such as Cassandra for high availability). This requires all features to be computed and stored in these data-stores. For training, we read from offline DB, for serving we read from online DB. There are two tricky aspects to this: 1. maintaining the two data stores in sync 2. making sure same computation logic computes features for both\nThere’s also a limitation that when you have to add new features you’ve to do a massive backfill (usually manually) and make it available offline for training. This backfill is needed because usually a model is trained on large historical time window (like past 3 months).\nMost Feature Store infrastructure use this approach and solve the above-mentioned issues: shared computation logic and auto-backfills. Big tech companies in ML space (Facebook, Uber, Airbnb) have developed some sort of DSL for shared computation logic so developers don’t have to write the code twice, which is pretty neat. The DSLs are usually very domain and company specific so I am not sure how much of it can be widely adopted even if they open-source it.\nAirbnb via Zipline provides an efficient way to perform backfill for different types of aggregations. Check out this video for how they do it.\n\n\nTime travel\nThat sounds like sci-fi but it basically means, given a point of time in the past, what was the value of this particular feature at that point of time. DS/MLE often need this to create their dataset where they use existing labels at different point in time and collect historical aggregates from past until this point-in-time to create the ML models. Getting this right is really important to avoid label leakage.\nThis is often not available in traditional DB solutions unless for each schema that contains our features, we add an event-time column. And one can image this data growing to be super large to be stored in a traditional DB.\nA more appropriate solution in this case is storing your snapshot or changelogs partitioned by time on Datalake. So we can store all the updates partitioned by date/time and we can write Spark/SQL queries on top of it to answer question such as: what was the average rating given by this user from January 2020 to March 2020.\nThankfully this is provided as ready-to-use solutions by AWS (using a bunch of AWS solutions such as S3, Glue, Athena, Kinesis), and Databricks Delta. Delta can work on top of your existing Datalake (S3, Azure blog storage, HDFS) and provide good data management solution on top of it.\nA challenging aspect of this is to manage streaming data with change-logs on your datalake. Delta by Databricks provides a solution to this problem as well. One option is to not perform streaming updates, and run batch-incremental updates frequently on your changelogs, and this is usually good enough. For e.g. user’s last 90 days of historical aggregate (average 90 day CTR of a user) won’t change much every hour so it’s alright to do it on a snapshot once per day. However, if last 10 searches made by the user is important for your model then you need streaming updates.\n\n\nFeature Engineering\nThis problem is very closely related to consistency problem, but this sits more close to DS/MLE rather than infrastructure folks. The problem is how to find a good set of libraries to do feature engineering and also have your feature definitions be consistent. For e.g. there are libraries such as Pandas and PySpark which DS/MLE use for feature engineering and these are pretty powerful, but they have their subtle differences which can go unnoticed. So if one DS is computing one-hot encoding in Pandas, that might differ from one-hot encoding done in PySpark. So how do we provide a single interface for all DS/MLE to do feature engineering.\nOption 1: Provide a DSL\nThis is a considerable effort in my opinion, especially for small companies with a small engineering workforce, because it is partially reinventing the wheel. It has great returns (I believe) in the form of consistency between features, ease of use and deployment, ease of customization, improved iteration and development of new ML models. Big companies like Uber, Airbnb, Twitter have taken this approach and it has paid off well for them. They have thousands of ML models running in production created by hundreds of engineers. It’s upto us to figure out whether this massive investment will pay off.\nOption 2: Use a single feature-engineering library\nThis can be limiting: what if this one library is not enough for your feature engineering needs? And can this library be used for serving too? Some solution around this is to have your feature engineering library be in a language that can be used for serving and provide wrappers on top of it for interactive languages such as Python. I believe this is what Twitter does by providing wrappers on top of Scala (I am not a 100% sure).\nOption 3: Shareable format and execution\nIf we can come up with a format and execution engine that can take my feature engineering pipeline written in Python using Scikit-learn and serialize it into a format that can be used by my realtime inference service (possibly JVM based), then this problem is solved!\nAt Yelp, we have converged towards this solution. We don’t limit our DS/MLE to use a single library like Pandas or PySpark but any library/feature engineering transforms as long as they can be serialized into a shareable format: MLeap. For more about Yelp’s ML platform, check out this blog. MLeap is relatively new but really powerful as it allows us to go between different training and serving environments and not limiting ourselves by having to use only one single library for feature engineering. It is also really fast as can be seen here in the benchmarks.\nThere are caveats to this: not all transforms are available and serializable, but this is a growing community and I believe we’ll have a good range of feature engineering transforms available soon.\nApache Beam is also a solution in a similar vein that it decouples your computation logic (batch/streaming) with your serving environment by using a shareable Beam Model. I have not played around with this yet.\n\n\nReusable Dataset:\nMost of the use-cases we discussed above (CTR prediction, search ranking, homefeed ranking, fraud detection) have a shared underlying theme: the underlying data changes very often and hence you really need to retrain your models often. To reduce the time to retrain and deploy fresher models, an often used solution is to materialize your dataset with newly arriving data points regularly, and then let your training batch pick up newly created rows, train a new model, evaluate it against older model and go into production. This is different from having a generalized Feature Store since datasets are very domain specific: a dataset created for CTR prediction cannot be used for ranking search results. However, they can share some same features (say user’s preference towards an item category) from the generalized Feature Store infrastructure.\nUsually, long term cheap storage such as S3/HDFS is used to materialize datasets. It is also a good idea to materialize data in a format that’s suitable for your model-training library (TFRecord for Tensorflow, .npy for PyTorch, .h5 for Keras, parquet for Spark ML models).\n\n\nOnline serving\nWhen DS/MLE train the models, their feature set usually comprises of hundreds of features taken from different DW/Datalake tables. But in the serving environment, a request consists of very basic information such as user_id, product_id, time_of_request, user’s platform (mobile/web), etc. The next important step is to hydrate this request to create a feature vector, in order to make predictions. This is where the online-feature-store comes in. It is usually keyed by your primary business entities (user, business, product) and consists of number of attributes/features about your entities using which you can create your feature vector. Sometimes additional transformation on top of these features is necessary. This is where Feature Engineering comes into play. Finally, the feature-vector is then sent to the model to serve real-time predictions. Apache Cassandra is often used to store features for online retrieval due to high throughput, availability, and scalability. Some other popular choices are MySQL and MongoDB.\n\n\nMetadata management\nThe whole point of building a shareable feature store is to: 1. reduce the duplication of effort and feature computation between teams 2. have consistent definition of features (user_7d_clicks means the same thing to all the teams).\nBut how does one team discover features created by another? That’s where metadata management comes in. We need a system that manages metadata about our features and use this metadata to serve a web UI using which developers can discover and use existing features.\nIn a way, we want to build a marketplace of features where teams can create/use features from other teams. And just like any other marketplace, we need some key pieces of attributes about the products of this market: features. Some important attributes to know are: - who produced this feature (who is responsible) - who are the consumers of this feature (how popular is this feature, which model/dashboard is this feature powering) - what is the SLA of this feature (offline and online) (what guarantees does it come with) - how often is this feature refreshed/updated (how well maintained is it) - data lineage (what upstream log/data does this feature use)\nIn addition, we can build APIs on top of this metadata database to easily get and put features in the feature store.\n\n\nData governance\nAn often ignored aspect is data governance, i.e. having some security and monitoring layer on top of your features. Who has access to what should be controlled especially if you are a global business and working in environments with different restrictions (for e.g. using health data). This is also necessary to manage SI data.\nSome data governance solutions provide profiling and cataloging capabilities such that different governance policies can be applied to different levels of sensitive data. Regular audits, quality checks, and monitoring is also a part of data governance. Lastly, encryption, data masking, and deletion of some data can also an important aspect especially if these data can introduce harmful bias in your ML systems.\n\n\n\nConclusion\nThis was a lot to take I imagine. But that’s because the problem of feature and data management in ML systems is not trivial. Above mentioned are only some of the main issues that slow down the iteration speed of organization to ship ML-based products. A well-maintained feature-store infrastructure comes a long way to shorten the ML lifecycle and make DS/MLE productive. It makes the life of DS/MLE easy by allowing them to focus on translating business use-cases to well-defined ML problems, perform feature engineering, build models, and deploy it without a lot of hassle.\nIn this post, we saw some of these issues and briefly discussed some possible solutions as well. In the next post, I will dive into a couple of commercially available feature stores, and perhaps also discuss some in-house feature stores by Uber and Airbnb. We’ll see how these feature-stores try to solve the problems around managing ML data and analyze some trade-offs.\n\n\nReferences\n\nFeature Store website\nHopsworks: open source Feature Store\nTecton.AI: commercial Feature Store\nYelp ML Platform\nYelp Ads ML Infrastructure for feature consistency\nGoogle’s paper on hidden technical debt in ML systems"
  },
  {
    "objectID": "posts/llm-from-scratch/token-embeddings.html",
    "href": "posts/llm-from-scratch/token-embeddings.html",
    "title": "LLMs Part 1: Token Embeddings",
    "section": "",
    "text": "In this post, I am diving into token embeddings. LLMs like any other ML model cannot process raw-text directly. The process with which we convert raw-text into acceptable format (i.e. vector of numbers) is loosely called tokenization. Once tokenized, LLMs learn contextual embeddings for these tokens. We learn embeddings as part of LLM pre-training because doing so makes embeddings optimized for the specific data and learning task. The same concept applies to other data formats: audio, video, etc.\nWe perform this conversion in following steps: 1. Tokenization 2. Mapping to Token IDs 3. Adding special context tokens 4. Byte-pair encoding 5. Generate token dataset for GPT-style model 6. Token Embedding 7. Position Embedding\n\nTokenization\nA simple way to do this is to use some kind of regex expression to split your text into individual words. A lot of design decisions come into play here, for example: - do we keep text case sensitive? - Might be good if your LLM is supposed to generate case sensitive text - do we keep and encode whitespace characters? - Might be good if your LLM is used for applications where whitespace generation is important: Python code generation - do we want to keep punctuation? - Might be good if your LLM is used for expressive text generation but maybe not if you are doing simpler tasks like text classification\nIf we use both cases, whitespaces, punctuation, etc. we increase computational complexity but get more expressability in our tokens. In most modern NLP approaches (with LLMs), we keep all characters (rather we algorithmically decide using sub-words: more details later in this post)\nAfter we tokenize the data, we typically end up with more words than if we had just split by whitespaces. Ok, so now instead of a single string we have a list of strings, but this is not acceptable to LLMs either, so we now generate an identifier for each token.\n\n\nMapping to Token IDs\nEssentially, we find all the unique tokens and assign them an integer ID to generate a map from token (str) -&gt; tokenID (int). We also want to keep a reverse-map of this so that we can map IDs back into tokens. The size of this mapping defines the “vocabulary size”. Vocabulary size is the number of unique tokens that your LLM can generate. Tokenizer is now capable of 1) encoding: converting a string to a list of tokenIDs using a vocabulary and 2) decoding: converting a list of tokenIDs into list of tokens and subsequently a string. Since, we are working with map, one obvious question is what happens if we a Key does not exist in the map. What this means in our tokenization context is what if we get a string that our Tokenizer has not seen before? It won’t be able to convert it to TokenID, and hence it will fail to tokenize any text with words/strings it has not seen before. Similar to how we handle unknown keys in map, we can come up with some default value for unknown keys.\n\n\nSpecial context tokens\nSpecial context tokens are essentially a catch-all to cover our corner cases (like mentioned above) and to provide LLM further assistance in text generation and understanding. For example, we can add an &lt;unk&gt; token to our vocabulary which acts as as a default value when our tokenizer seens an unknown token. Another common token is &lt;eos&gt; to denote end of sequence (or rather to indicate to LLM that it can stop generating text) It is useful when giving LLMs multiple sentences/documents that are unrelated. Some other common special context tokens are: &lt;bos&gt;: beginning of a new sequence and &lt;pad&gt; to denote that these are just padding to fit the batch size during training.\nWe basically add these special tokens to our vocabulary map and reverse-map and now our tokenizer is ready to tokenize any string! The downside of our existing tokenizer is the following: - It is terrible at unseen words. For example, if “come” is in our vocabulary but “coming” is not, there’s no way for our tokenizer to encode “coming” even though it is very closely related to a known token “come”. So all unseen words become &lt;unk&gt;, even if we have closely related words in the vocabulary - Our tokenizer relies on splitting over whitespace which works fine for English but not for all languages\n\n\nByte pair encoding\nByte pair encoding, commonly called BPE is a subword-based tokenization scheme. On a high level, this is what it does: 1. Split words into individual characters (unigrams) 2. Merge commonly occurring pair of adjacent characters into new tokens 3. Repeat 2 until some stop condition like size of vocabulary or frequency cut-off So, instead of word-based tokenization which is what we discussed earlier, we get “sub-word” based tokenization. Conceptually: character &lt; sub-word &lt; word. The subword based tokenization does not split frequently occurring words but instead splits rarely occurring words. So “toy” may remain as a token but “toys” is split into “toy” and “s”. So, now when our tokenizer seens an unknown word while encoding, it splits into into tokens it has seen before. For e.g. if we are trying to tokenize a word: “absobloodylutely”, it might break it up into , ,  subword tokens (assuming these 3 are present in the vocabularly generated by BPE tokenizer).\nTODO: I’ll dive into BPE and sub-word tokenization is discussed in a separate blog post.\n\n\nGenerate token dataset for GPT-style model\nBefore we go about creating embeddings for token, we need a way to create a “task” that will serve to create these embeddings. How GPT-style decoders typically go about doing so is some sort of variation of next-token prediction. So, we have a sentence: “I am a writer” in our text corpus. We can create training samples with &lt;data, label&gt; pair like below: - [I], am - [I, am], a - [I, am, a], writer So essentially, we have to employ a sliding window approach through our corpus and create these pairs. One common design choice is what is the maximum length of my feature-vector. This is called the context-length. Higher context-length means more memory/compute needed during training as well as inference. However, the LLM can process and reason through a much longer piece of text, i.e. it can understand more context. Shorter context length could work if you are creating very targeted use-case like sentiment analysis of product reviews where reviews are short and the label generated is also just a sentiment label. Most GPT-style/decoder-style LLM models come with massive context length so that they are are as powerful as possible. To do this in practice, we can create a Dataset and DataLoader class in PyTorch. Dataset would convert our text into tokens using a tokenizer and create input_ids and target_ids by shifting the sliding window till it reaches a size of context_length. So, now our dataset looks like a row of tensors where our input_ids look like: [30,23,1000,12] and target_ids look like: [23,1000,12,6] (see how we set stride of 1 here).\n\n\nGenerate Token Embeddings\nTo go from a list of tokenIDs to a list of token embeddings, we have to create an Embedding matrix of shape [vocab_size, embedding_size] where embedding_size is a design choice. Larger embedding_size means more compute/memory required but we also get a more powerful model. If we look at the shape of the Embedding matrix, we can say that each token is represented by a vector of size embedding_size and this Embedding matrix is basically a look-up table to go from token-id to a token-embedding. But instead of doing a naive one-hot encoding lookup, we do a lookup over a continuous space of embedding_size. This matrix is initialized randomly and is learned during the process of LLM pre-training.\nWe should be done now, right? We started with a piece of text, cleaned it, tokenized it into subword based tokens, and now we have a vector representation that can be fed into a neural network. But, there’s a crucial piece missing. Modern transformer based LLMs are based on attention-mechanism. This attention mechanism works on tokens independently as in it does not have a notion of position of tokens in a piece of text. So, if a token 22 appears in text, it will always be mapped to the same embedding irrespective of where it appeared in the sequence.\nTo get around this shortcoming, position embeddings were introduced.\n\n\nGenerate Position Embeddings\nThere are 2 categories of position embeddings: absolute and relative. Absolute position embeddings are defined by exact position of the token. So, any token at first position always gets the same position embedding irrespective of how long is the sequence. Relative position embeddings depend on a token’s position w.r.t other tokens in the sequence, i.e. distance between tokens in a sequence. So the model learns how far apart are certain tokens rather than where exactly a token is situated. So relative works pretty well with varying size sequences and sequence lengths that are unseen in training. For absolute position embedding, the shape of this matrix would be [context_length, embedding_size]. Note that the input to position embedding layer is essentially a placeholder vector containing each position from 0 to context_length. Position embeddings can also be learned during training process, this is how it works in GPT models. T5 model uses relative position embedding whereas GPT/LLama use a Rotation based absolute position embedding.\nFinally, to create the input embeddings we add token embeddings and position embeddings!\ntext -&gt; token IDs -&gt; token_embedding\n[0,...,context_length] -&gt; position_embedding\ninput_embedding = token_embedding + position_embedding\n\n\nSummary\n\nLLMs like any other NNs need numbers to work with, so we need to convert text to numbers\nWe can split the text into subword based tokens\nNext, we learn a continuous representation for these tokens and call it token embedding\nSince attention based LLMs are position agnostic, we add a position embedding to token embedding to obtain the input embedding of a token"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "LLMs Part 2: Attention\n\n\n\n\n\n\nml\n\n\ndeep-learning\n\n\nllm\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nRahul Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs Part 1: Token Embeddings\n\n\n\n\n\n\nml\n\n\ndeep-learning\n\n\nllm\n\n\n\n\n\n\n\n\n\nDec 25, 2024\n\n\nRahul Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Store: an essential part of ML infrastructure\n\n\n\n\n\n\nml\n\n\nfeature-store\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\nRahul Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Pythonic code: Part 2\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nRahul Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Pythonic Code: Part 1\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nRahul Dubey\n\n\n\n\n\n\nNo matching items"
  }
]